1.Yao et-al:
1)the last fully-connected layer can be used as a fixed-size vector representation,or the feature map of the last convolutional layer can be used as a set of spatial feature vectors.
2)Srivastava et al.propose to use LSTM to extract video features.Ranzato also use RNN to do that.so RNN can do that work too.

2.Venugopalan-Improving LSTM-based ...:
1)this paper use the external corpora to incorporate with the traslation model.
2)有大概三种方法将这两种model结合在一起，分别是：early fusion,late fusion,deep fusion.
3)这篇论文引用了Gulcehre2015年的方法，他们不同的地方是，在使用deep fusion 方法的时候，去掉了Attention机制.

3.Heilbron et at:Activity,A large-scale video....:
1)这是一篇关于human activity数据集的paper，主要介绍了ActivityNet这个数据集。
2)hollywood：一个视频数据集，这篇文章还介绍了许多其他种类的数据集。

4.Krishna et al:Dense-caption events in videos:
1)they introduce a captioning module that utilizes the context from all the event from there module to generate ech sentence.
2)公开可一个activity caption的数据库。
3)使用了一个DAPs的变形model，使用不同stride来捕捉长度不一的feature.
4)提到了一个叫做Visual Genome的image caption数据库。

5.Sami et al: YouTube-8M:A large-scale video classification benchmark
1)介绍了一个包含8M video 的数据库，提到一个叫做sports-1M的数据库，可以参考一下。
2)一个骑自行车的视频，如果周围的环境不同，那么它可能是不同的运动，比如山地自行车，比如公路自行车。
3)这个数数据集合可以

6.Xiaoqiang Lu et al--Exploring Models and Data for remote Sensing Image Caption Generation. 介绍了一个遥感地图的数据集,可以考虑使用.方法还是老方法.




